---
title: "Credit Card Customer Analysis"
author: "Henry Siegler, Jake Ketchner, Esteban Anderson, Alex Fugate"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning = FALSE,
                      message = FALSE)
```

```{r}
library(tidyverse)
library(here)
library(vtable)
library(reshape2)
```

## Introduction

This report is an investigation analyzing data on credit card customers for a particular company to learn about the relationship between various attributes of the credit card customers, such as their spending habits and whether or not they are still current customers. The data was obtained from kaggle [at this link](https://www.kaggle.com/code/atillazkaymak/credit-card-customer-churn-prediction/data?select=BankChurners.csv). The dataset is from an unknown credit card company and we only are provided data for some of their customers that use of used to use their credit card to some degree. We have 10,127 total observations. Each observational unit in the study is a customer of the credit card company, and the variables that we are focusing on are total transaction amount in the past 12 months (in dollars), total transaction count in the past 12 months, whether or not the customer is still with the company (binary), total number of products held by the customer, and the customer's credit limit (in dollars). For the simple and multiple linear regression sections, we are considering total transaction count in the past 12 months the response variable. For the logistic regression section, whether or not the customer exited is the response variable, which is a binary variable equal to 1 if the customer exited. Initially, we hypothesize that total transaction amount and total transaction count would be positively and linearly related, as it makes sense for increased spending to be associated with an increased number of transactions. We hypothesize that total transaction count to be negatively correlated with if the customer exited, and positively correlated with both credit limit and total number of products held by the customer. 

```{r}
data <- read_csv(here("BankChurners.csv", "BankChurners.csv"))
```

```{r}
data <- data %>% 
  mutate(Exited = case_when(Attrition_Flag == "Existing Customer" ~ 0,
                              Attrition_Flag == "Attrited Customer" ~ 1),
         male = case_when(Gender == "M" ~ 1,
                          Gender == "F" ~ 0),
         ) %>% 
  select(-Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_1,
         -Naive_Bayes_Classifier_Attrition_Flag_Card_Category_Contacts_Count_12_mon_Dependent_count_Education_Level_Months_Inactive_12_mon_2,
         -Gender,
         -Attrition_Flag,
         -CLIENTNUM)
```

## Descriptive Statistics
 
__Response Variable is Total Transaction Count__

```{r}
descriptive_stats <- data %>% 
  select_if(is.numeric) %>% 
  st(out = "return")

descriptive_stats
```

In the table above, we can see some descriptive statistics of the variables in our dataset. None of the numeric variables have extremely large maximum values.

```{r}
data %>% 
  select_if(is.numeric) %>% 
  cor() %>% 
  round(2) %>% 
  melt() %>% 
  ggplot(aes(Var1, Var2, fill = value)) + 
  geom_tile(color = "white") + 
  scale_fill_gradient2(low = "blue", high = "red", mid = "white", 
                       midpoint = 0, limit = c(-1,1), space = "Lab", 
                       name="Pearson\nCorrelation") +
  theme_minimal() + 
  theme(axis.text.x = element_text(angle = 45, vjust = 1.1, size = 8, hjust = 1.1),
        axis.title.x = element_blank(),
        axis.title.y = element_blank()) + 
  coord_fixed() + 
  labs(title = "Correlation Matrix of All Numeric Variables")
```
We can see from the correlation matrix that __Avg_Open_To_Buy__ is highly correlated with __Credit_Limit__, so we will remove __Avg_Open_To_Buy__ because it is redundant to keep both of these variables. We can see that __Total_Trans_Ct__ is highly correlated with __Total_Trans_Amt__, which is what we would expect. __Total_Trans_Ct__, the response variable, is moderately negatively correlated with __Total_Relationship_Count__, which is not what we would expect, because we would expect customers with more products with the company to have more transactions. The response is also moderately negatively correlated with __Exited__, which makes sense because we would expect the customers who are not using the company now to have less total transactions.

```{r}
cor <- cor(data$Avg_Open_To_Buy, data$Credit_Limit)
cat("Correlation between Avg_Open_To_Buy and Credit_Limit:", cor)
```
```{r}
data <- data %>% 
  select(-Avg_Open_To_Buy)
```

```{r}
cor_matrix <- cor(data %>% 
                    select_if(is.numeric)) %>% 
  round(2)

cor_matrix %>% 
  as.data.frame() %>% 
  select(Total_Trans_Ct) %>% 
  arrange(Total_Trans_Ct)
```

Looking at the actual correlations between the response variable and all of the explanatory variables, we can see that the variables with the highest correlation are __Total_Trans_Amt__, __Exited__, __Total_Relationship_Count__, and __Contacts_Count_12_mon__.

Now we will explore the __Total_Trans_Ct variable__ and its relationship with other correlated variables in our data. 

```{r}
data %>% 
  ggplot(aes(x = Total_Trans_Ct)) + 
  geom_histogram(fill = "dodgerblue") + 
  labs(x= "Total Transaction Count",
       title = "Distribution of Total Transaction Count")
```

We see that most customers have a total transaction count in the range of about 30 to 90 transactions in the past year.

```{r, fig.width = 3, fig.height = 6}
data %>% 
  ggplot(aes(x = Total_Trans_Ct)) + 
  geom_histogram() + 
  facet_wrap(~ Income_Category, ncol = 1) + 
  labs(x = "Total Transaction Count")
```

For all of the income categories, we see a bimodal distribution for the __Total_Trans_Ct__. For people in the income category of "less than $40K", we see that there is a larger spike in the second peak of the histogram compared to the other income categories. However, looking at the general distribution of total transaction count across the different income levels, the distributions look pretty similar and have similar centers and averages, so it does not appear that income category is related to the response.

```{r}
data %>% 
  ggplot(aes(x = Total_Trans_Ct)) + 
  geom_histogram() + 
  facet_wrap(~ Exited, nrow = 2) + 
  labs(x = "Total Transaction Count",
       title = "Distribution of Total Transaction Count by Exit Status (1 means Exited)")
```

We can see that the Total Transaction Counts for the customers who have have exited has a unimodal distribution, with the center of the peak being significantly below where most of the Transaction Counts are for the customers who have not exited. The average total transaction count for those who have exited appears to be about 35 transactions, but that number is around 70 for those who have not exited. Therefore, exited does appear to be very related to the response.

```{r}
data %>% 
  ggplot(aes(x = as.factor(Total_Relationship_Count),
             y = Total_Trans_Ct)) + 
  geom_bar(stat = "summary",
           fun = mean,
           fill = "dodgerblue") + 
  labs(x = "Total Number of Products the Customer Has", 
        y= "Average Total Transaction Count")
```
The average total transactions is smaller for the groups of people with 3, 4, 5 and 6 products compared to those with only 1 or 2 products. Total Relationship Count (total number of products a customer has), appears to have a relationship with the response.

## Data Visualization

```{r}
data %>% 
  ggplot(aes(x = Total_Trans_Amt, y = Total_Trans_Ct, color = as.factor(Exited))) + 
  geom_point(alpha = 0.3) + 
  geom_smooth(method = "lm", formula = y ~ poly(x, 3), se = FALSE) + 
  labs(x = "Total Transaction Amount", 
       y = "Total Transaction Count",
       color = "Exit Status",
       title = "Third Degree Polynomial Smoother for Customers who did and did not Exit") + 
  scale_color_manual(labels = c("Did not Exit", "Exited"), values = c("dodgerblue", "orange"))
```

Based on the scatterplot between total transaction count and total transaction amount, we can see that the data appears to form 3 clusters, and there are no customers who exited that were in the 3rd cluster. Also, the data for both customers who exited and did not exit appears to follow a 3rd degree polynomial relationship.

## Data Cleaning

From our descriptive statistics earlier, we found that there were no extremely large values for any of the variables, so we have no evidence that any of the values in the dataset are incorrect entries.

Running a simple linear regression model predicting total transction count from total transaction amount, we see the following Cooks D values:

![Alt text](C:\Users\hasie\Desktop\STATProject\SL12.jpg)

All of the Cook's D values are very low, because they are much lower than 1. However, a few observations do stick out more than others; 2 of the observations have values of about 0.008. 

```{r}
descriptive_stats %>% 
  filter(Variable == "Total_Trans_Amt" | Variable == "Total_Trans_Ct")
```

We can see that these two individuals have very high total transaction amounts relative to the dataset as a whole. They have values of \$16,563 and \$17,744, which are well above the 75th percentile, and much closer to the maximum value of \$18,484. Their total transaction counts are also very high: 94 and 104. These transaction counts are higher than the 75th percentile for the data of 81. The very high values for both the explanatory variable and the response for these individuals explains why the Cook's D value is so high when a linear regression is ran of these two variables. We do not have any reason to remove those observations because their values are correct and they are not extremely high.

## Splitting the Data

```{r}
set.seed(9)
data$id <- 1:nrow(data)

training_data <- data %>% 
  sample_frac(0.7)

test_data <- data %>% 
  anti_join(training_data, by = 'id')
```

We randomly selected 80% of the observations and placed those into a training dataset. The remaining 20% of the data were placed into a testing dataset.

# Linear Regression

Explanatory Variable: __Total_Trans_Amt__
Response Variable: __Total_Trans_Ct__

```{r}
training_data %>% 
  ggplot(aes(x = Total_Trans_Amt, y = Total_Trans_Ct)) + 
  geom_point(alpha = 0.3, color = "dodgerblue") + 
  geom_smooth(method = "lm", color = "red") + 
  labs(x = "Total Transaction Amount",
       y = "Total Transaction Count")
```

As we can see in the scatter plot, the relationship between total transaction count and total transaction amount is not linear, so we must apply transformations to make the relationship linear. 

## Variable Pre-Processing

### Attempt #1

First, we will try decreasing the power of the X variable, by taking the square root. Also, we will increase the power of Y, by squaring the Y variable.

```{r}
training_data %>% 
  ggplot(aes(x = Total_Trans_Amt^0.5, y = Total_Trans_Ct^2)) + 
  geom_point(alpha = 0.3, color = "dodgerblue") + 
  labs(x = "Square Root Total Transaction Amount",
       y = "Total Transaction Count Squared") + 
  geom_smooth(method = "lm", color = "red")
```

Linearity looks better, but equal error variance does not appear to be satisfied. We have increasing variance, so we want to decrease the power of Y to fix the unequal error variance. We will first try the square root of Y instead of Y squared.

### Attempt #2

```{r}
training_data %>% 
  ggplot(aes(x = sqrt(Total_Trans_Amt), y = sqrt(Total_Trans_Ct))) +
  geom_point(alpha = 0.3, color = "dodgerblue") + 
  labs(x = "Square Root Total Transaction Amount",
       y = "Square Root Total Transaction Count") + 
  geom_smooth(method = "lm", color = "red")
```

Equal error variance looks like it is satisfied, however linearity does not look ideal. Therefore, we want to try to find a transformation for Y that is between $Y^{0.5}$ and $Y^2$. 

### Attempt #3

```{r}
training_data %>% 
  ggplot(aes(x = sqrt(Total_Trans_Amt), y = Total_Trans_Ct^0.8)) + 
  geom_point(alpha = 0.3, color = "dodgerblue") + 
  labs(x = "Square Root Total Transaction Amount",
       y = "Total Transaction Count to the Power of 0.8") + 
  geom_smooth(method = "lm", color = "red")
```

Raising total transaction count to the power of 0.8 keeps the relationship fairly linear and maintains fairly equal error variance. However, linearity does not look perfect, so let us continue to decrease the power of X

### Attempt #4

Decreasing the power of X from 0.5 to 0.1, we get the following:

```{r}
training_data %>% 
  ggplot(aes(x = Total_Trans_Amt^0.1, y = Total_Trans_Ct^0.8)) + 
  geom_point(alpha = 0.3, color = "dodgerblue") + 
  labs(x = "Total Transaction Amount to the Power of 0.1",
       y = "Total Transaction Count to the Power of 0.8") + 
  geom_smooth(method = "lm", color = "red")
```
These transformations seem to do the best job at linearizing the data, while retaining equal error variances.

## Residual Analysis

Therefore our current best transformations for simple linear regression between __Total_Trans_Amt__ and __Total_Trans_Ct__ are: 

X' = $Total\_Trans\_Amt^{0.1}$
Y' = $Total\_Trans\_Ct^{0.8}$

__Equal Error Variance__

```{r}
model <- lm(I(Total_Trans_Ct^0.8) ~ I(Total_Trans_Amt^0.1), data = training_data)
training_data$predictions <- predict(model)
training_data$residual <- training_data$Total_Trans_Ct^0.8 - training_data$predictions
```

```{r}
training_data %>% 
  ggplot(aes(x = predictions, y = residual)) + 
  geom_point(color = "dodgerblue", alpha = 0.3) + 
  labs(x = "Predicted")
```
![Alt text](C:\Users\hasie\Desktop\STATProject\SL2.jpg)

Based on the residual by predicted plot and the Brown-Forsythe small p value, we can conclude that equal error variance is not satisfied. There residuals seem to fan out as the predicted value increases.

__Linearity__

![Alt text](C:\Users\hasie\Desktop\STATProject\SL1.jpg)

Linearity also does not look to be fully satisfied, with an F Ratio of 1.547 and a very small p value. Two of our assumptions are already violated so we will try new transformations. 

### New Transformation

```{r}
training_data %>% 
  ggplot(aes(x = Total_Trans_Amt^-0.2, y = Total_Trans_Ct^0.7)) + 
  geom_point(alpha = 0.3, color = "dodgerblue") + 
  labs(x = "Total Transaction Amount to the Power of -0.2",
       y = "Total Transaction Count to the Power of 0.7") + 
  geom_smooth(method = "lm", color = "red")
```

__Equal Error Variance__

```{r}
model <- lm(I(Total_Trans_Ct^0.7) ~ I(Total_Trans_Amt^-0.2), data = training_data)
training_data$predictions <- predict(model)
training_data$residual <- training_data$Total_Trans_Ct^0.7 - training_data$predictions
```

```{r}
training_data %>% 
  ggplot(aes(x = predictions, y = residual)) + 
  geom_point(color = "dodgerblue", alpha = 0.3) + 
  labs(x = "Residual by Predicted Plot")
```

![Alt text](C:\Users\hasie\Desktop\STATProject\SL3.jpg)

With this new transformation, we now see that the equal error variance assumption is now satisfied based on the large p value for the Brown-Forsythe test.

__Linearity__

![Alt text](C:\Users\hasie\Desktop\STATProject\SL4.jpg)

We still cannot conclude that linearity is satisfied based on the lack of fit test. However, the F ratio decreased to 1.34, which is an improvement from 1.547 in the previous model. The F critical value for these degrees of freedom is 1.057, so our F ratio is not too far above this. 

__Normality of Residuals__

![Alt text](C:\Users\hasie\Desktop\STATProject\SL5.jpg)

![Alt text](C:\Users\hasie\Desktop\STATProject\SL6.jpg)

Although we have a small pvalue for the Anderson-Darling test, which indicates that we do not have normality of residuals, the histogram of the residuals looks very normally distributed, so we can conclude that this assumption is not terribly violated.

__Independence__

![Alt text](C:\Users\hasie\Desktop\STATProject\SL7.jpg)

Our data on the customers is sorted in a random order, so there is no reason why the error of one observation would be related to the error of the observation next to it. The Durbin-Watson value is 1.955, which is very close to 2, which is the value that means there is no autocorrelation. Our Durbin-Watson value has a pvalue of 0.03, which is not significant at the alpha of 1% level. Since we know that our data is in a random order, and customers are likely not influencing each other, we can conclude that independence is satisfied.

### Unusual Observations

![Alt text](C:\Users\hasie\Desktop\STATProject\SL8.jpg)

There are many externally studentized residuals greater than 3 in absolute value. However, there is only 1 observation that is a large outlier based on the Bonferroni adjustment.

```{r}
training_data %>% 
  select(residual, Exited, Total_Trans_Amt, Total_Trans_Ct, Education_Level) %>% 
  slice_max(order_by = -residual, n = 10)
```
```{r}
training_data %>% 
  select(residual, Exited, Total_Trans_Amt, Total_Trans_Ct, Education_Level) %>% 
  slice_max(order_by = residual, n = 5)
```

```{r}
descriptive_stats %>% 
  filter(Variable == "Total_Trans_Amt" | Variable == "Total_Trans_Ct")
```

We can see that the customers with the largest negative residuals almost all exited the company and had high transaction amounts and lower transaction counts. For the customers with the largest positive residuals, none of them have exited the company, and they have much lower transaction amounts and high transaction counts.

![Alt text](C:\Users\hasie\Desktop\STATProject\SL9.jpg)

Looking at the Cook's D Influence values, only a couple of the observations stick out from the others. The Cook's D values are low, since values of 0.5 or greater indicate that the observation is influential. However, since one of the observations has a much higher Cook's D value than the other points, that observation is likely influential.

```{r}
training_data %>% 
  filter(id == 244) %>% 
  select(residual, Exited, Total_Trans_Amt, Total_Trans_Ct, Education_Level)
```
This customer has a very low total transaction amount and a low total transaction count, which may be the reason for why it has a Cook's D value so much greater than the other observations. 

## Fit a Linear Model

Final Model:

$X' = Total\_Trans\_Amt^{-0.2}$
$Y' = Total\_Trans\_Ct^{0.7}$

```{r}
training_data %>% 
  ggplot(aes(x = Total_Trans_Amt^-0.2, y = Total_Trans_Ct^0.7)) + 
  geom_point(alpha = 0.3, color = "dodgerblue") + 
  labs(x = "Total Transaction Amount to the Power of -0.2",
       y = "Total Transaction Count to the Power of 0.7") + 
  geom_smooth(method = "lm", color = "red")
```

The model above did not terribly violate any of the assumptions required for simple linear regression models, so the predictions and implications of our model should be fairly accurate and trustsworthy. The model does not account for omitted variable bias, which means that we cannot interpret the slope coefficient as a causual effect of transaction amount on transaction count.

```{r}
summary(model)
```

$\widehat{Total\_Trans\_Ct}_i'=51.94 -170.88 Total\_Trans\_Amt_i'$

The model does make sense contextually because the X variable was transformed so that it was raised to the power of -0.2, so the negative coefficient for the slope means that there is a positive relationship between the untransformed variables. The variables have been transformed greatly, so the coefficient estimates do not make sense to interpret.

![Alt text](C:\Users\hasie\Desktop\STATProject\SL10.jpg)

The model did a good job of explaining variation in transaction count, as the R-squared value is 0.8281. Therefore, 82.8% of the variation in the total transaction count transformed variable is explained by the model. The root mean square error of the model is 1.977, which is the typical deviation of the actual Y value from the predicted Y value.

To interpret the intercept of the estimated regression equation, we can say that the estimated total transaction count to the power of 0.7 is 51.9 when the total transaction amount is zero. This intercept does make sense in context, however the minimum value of total transaction amount in the data was $510, so it may not be meaningful. For a 1 unit increase in the tranaction amount to the power of -0.2, the transaction count to the power of 0.7 decreases by 170.88.

## Statistical Inference

$H_0$: The model is not significant.
$H_A$: The model is significant.

![Alt text](C:\Users\hasie\Desktop\STATProject\SL11.jpg)

The F ratio for the test of overall model significance is 34,217, which is very large, meaning that we can conclude that our model is significant.

### Confidence Interval and Prediction Interval

```{r}
new_data <- data.frame(Total_Trans_Amt = 6000)
predict(model, newdata = new_data, interval = 'confidence', level =0.95)
predict(model, newdata = new_data, interval = 'prediction', level =0.95)
```

We are interested in calculating confidence intervals for the mean of individuals with transaction amounts of $6000, because there is a large cluster of customers who spent about that much total.

For customers with transaction amounts of $6000, we are 95% confident that the true mean transaction count to the power of 0.7 is between 21.88 and 22.004.

For a single new customer with a transaction amount of $6000, we are 95% confident that that customer's actual transaction count to the power of 0.7 is between 18.06 and 25.82.

## Model Validation

```{r}
test_data$predictions <- predict(model, newdata = test_data)
test_data$Ytrans <- test_data$Total_Trans_Ct^0.7
cat("Correlation between testing data predictions and actual values", cor(test_data$predictions, test_data$Total_Trans_Ct^0.7))
```
Using the linear regression model that was built using the training data, we calculate the predicted values for all the observations in the testing data. The correlation between those predicted values and the actual response values for the testing data is 0.9109, which is a very high correlation, meaning that our model still did a good job at predicting out of sample values.

## Conclusion

In order to investigate the relationship between transaciton amount and transaction count over the past 12 months for all of the customers of the credit card company in the dataset, we first had to transform our variables so that the assumptions for simple linear regression were met. Linearity and equal error variance were clearly not met before we applied transformations to the variables. After transforming the variables, equal error variance was satisfied, linearity was very close to being satisfied even although the lack fit test was not passed, and normality of errors looked satisfied. The final transformed model did a very good job in explaining variation in the response, with an R squared value of 0.8281. The model utility test demonstrated that our model is very significant, and we see that we have very statistically significants for the coefficient estimates for the intercept and the explanatory variable. There were not any extremely large residuals in our model, and there was only a couple of observations that could have been influential. 

In terms of finding the best transformations, many different combinations of transformations were attemped, and none of them seemed to fully remedy the linearity assumption, however the data does seem to have a linear form, so any conclusions made from the model should be close to accurate, as no assumptions were highly violated. We would have liked to have found a model that satisfied the lack of fit test, however we reduced the F ratio for this test to be not too high, which we felt was the best we could have done. We expected a positive correlation between these two variables, which is what we discovered. We found that the largest residuals in absolute value in our model were negative residuals, meaning that the model over predicted the response for these observations. We found it interesting that almost all of the observations with the largest negative residuals were individuals that had exited the credit card company. We found it interesting that one of the observations had a much higher Cook's D value than the others, but this customer had the minimum total transaction amount at $510.

One thing that we were not fully content with in our analysis is that the customers that we received information on were not all of the customers of the credit card company. The minimum value of the total transaction amount variable in the dataset was $510, so these customers all used their cards a reasonable amount. Also, there were no customers with extremely large values for any of the variables, such as transaction amount or credit limit, so it would have been nicer to know more about how this data was collected. 









